{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"xgbse : XGBoost Survival Embeddings \u00b6 \"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.\" - Leo Breiman, Statistical Modeling: The Two Cultures Survival Analysis is a powerful statistical technique with a wide range of applications such as predictive maintenance, customer churn, credit risk, asset liquidity risk, and others. However, it has not yet seen widespread adoption in industry, with most implementations embracing one of two cultures: models with sound statistical properties, but lacking in expressivess and computational efficiency highly efficient and expressive models, but lacking in statistical rigor xgbse aims to unite the two cultures in a single package, adding a layer of statistical rigor to the highly expressive and computationally effcient xgboost survival analysis implementation. The package offers: calibrated and unbiased survival curves with confidence intervals (instead of point predictions) great predictive power, competitive to vanilla xgboost efficient, easy to use implementation explainability through prototypes This is a research project by Loft Data Science Team , however we invite the community to contribute. Please help by trying it out, reporting bugs, and letting us know what you think!","title":"Home"},{"location":"index.html#xgbse-xgboost-survival-embeddings","text":"\"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.\" - Leo Breiman, Statistical Modeling: The Two Cultures Survival Analysis is a powerful statistical technique with a wide range of applications such as predictive maintenance, customer churn, credit risk, asset liquidity risk, and others. However, it has not yet seen widespread adoption in industry, with most implementations embracing one of two cultures: models with sound statistical properties, but lacking in expressivess and computational efficiency highly efficient and expressive models, but lacking in statistical rigor xgbse aims to unite the two cultures in a single package, adding a layer of statistical rigor to the highly expressive and computationally effcient xgboost survival analysis implementation. The package offers: calibrated and unbiased survival curves with confidence intervals (instead of point predictions) great predictive power, competitive to vanilla xgboost efficient, easy to use implementation explainability through prototypes This is a research project by Loft Data Science Team , however we invite the community to contribute. Please help by trying it out, reporting bugs, and letting us know what you think!","title":"xgbse: XGBoost Survival Embeddings"},{"location":"basic-usage.html","text":"API \u00b6 The package follows scikit-learn API, with a minor adaptation to work with time and event data ( y as a numpy structured array of times and events). .predict() returns a dataframe where each column is a time window and values represent the probability of survival before or exactly at the time window. # importing dataset from pycox package from pycox.datasets import metabric # importing model and utils from xgbse from xgbse import XGBSEKaplanNeighbors from xgbse.converters import convert_to_structured # getting data df = metabric.read_df() # splitting to X, y format X = df.drop(['duration', 'event'], axis=1) y = convert_to_structured(df['duration'], df['event']) # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors(n_neighbors=50) xgbse_model.fit(X, y) # predicting event_probs = xgbse_model.predict(X) event_probs.head() index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.98 0.87 0.81 0.74 0.71 0.66 0.53 0.47 0.42 0.4 0.3 0.25 0.21 0.16 0.12 0.098 0.085 0.062 0.054 1 0.99 0.89 0.79 0.7 0.64 0.58 0.53 0.46 0.42 0.39 0.33 0.31 0.3 0.24 0.21 0.18 0.16 0.11 0.095 2 0.94 0.78 0.63 0.57 0.54 0.49 0.44 0.37 0.34 0.32 0.26 0.23 0.21 0.16 0.13 0.11 0.098 0.072 0.062 3 0.99 0.95 0.93 0.88 0.84 0.81 0.73 0.67 0.57 0.52 0.45 0.37 0.33 0.28 0.23 0.19 0.16 0.12 0.1 4 0.98 0.92 0.82 0.77 0.72 0.68 0.63 0.6 0.57 0.55 0.51 0.48 0.45 0.42 0.38 0.33 0.3 0.22 0.2 You can also get interval predictions (probability of failing exactly at each time window) using return_interval_probs : # point predictions interval_probs = xgbse_model.predict(X_valid, return_interval_probs=True) interval_probs.head() index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.024 0.1 0.058 0.07 0.034 0.05 0.12 0.061 0.049 0.026 0.096 0.049 0.039 0.056 0.04 0.02 0.013 0.023 0.0078 1 0.014 0.097 0.098 0.093 0.052 0.065 0.054 0.068 0.034 0.038 0.053 0.019 0.018 0.052 0.038 0.027 0.018 0.05 0.015 2 0.06 0.16 0.15 0.054 0.033 0.053 0.046 0.073 0.032 0.014 0.06 0.03 0.017 0.055 0.031 0.016 0.014 0.027 0.0097 3 0.011 0.034 0.021 0.053 0.038 0.038 0.08 0.052 0.1 0.049 0.075 0.079 0.037 0.052 0.053 0.041 0.026 0.04 0.017 4 0.016 0.067 0.099 0.046 0.05 0.042 0.051 0.028 0.03 0.018 0.048 0.022 0.029 0.038 0.035 0.047 0.031 0.08 0.027 Survival curves and confidence intervals \u00b6 XBGSEKaplanTree and XBGSEKaplanNeighbors support estimation of survival curves and confidence intervals via the Exponential Greenwood formula out-of-the-box via the return_ci argument: # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors(n_neighbors=50) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) XGBSEDebiasedBCE does not support estimation of confidence intervals out-of-the-box, but we provide the XGBSEBootstrapEstimator to get non-parametric confidence intervals. As the stacked logistic regressions are trained with more samples (in comparison to neighbor-sets in XGBSEKaplanNeighbors ), confidence intervals are more concentrated: # base model as BCE base_model = XGBSEDebiasedBCE(PARAMS_XGB_AFT, PARAMS_LR) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=20) # fitting the meta estimator bootstrap_estimator.fit( X_train, y_train, validation_data=(X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS, ) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) The bootstrap abstraction can be used for XBGSEKaplanTree and XBGSEKaplanNeighbors as well, however, the confidence interval will be estimated via bootstrap only (not Exponential Greenwood formula): # base model base_model = XGBSEKaplanTree(PARAMS_TREE) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100) # fitting the meta estimator bootstrap_estimator.fit( X_train, y_train, time_bins=TIME_BINS, ) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) With a sufficiently large n_estimators , interval width shouldn't be much different, with the added benefit of model stability and improved accuracy. Addittionaly, XGBSEBootstrapEstimator allows building confidence intervals for interval probabilities (which is not supported for Exponential Greenwood): # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict( X_valid, return_ci=True, return_interval_probs=True ) # plotting CIs plot_ci(mean, upper_ci, lower_ci) The parameter ci_width controls the width of the confidence interval. For XGBSEKaplanTree it should be passed at .fit() , as KM curves are pre-calculated for each leaf at fit time to avoid storing training data. # fitting xgbse model xgbse_model = XGBSEKaplanTree(PARAMS_TREE) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS, ci_width=0.99) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) For other models ( XGBSEKaplanNeighbors and XGBSEBootstrapEstimator ) it should be passed at .predict() . # base model model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, N_NEIGHBORS) # fitting the meta estimator model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting mean, upper_ci, lower_ci = model.predict(X_valid, return_ci=True, ci_width=0.99) # plotting CIs plot_ci(mean, upper_ci, lower_ci) Early stopping \u00b6 A simple interface to xgboost early stopping is provided. # splitting between train, and validation (X_train, X_valid, y_train, y_valid) = \\ train_test_split(X, y, test_size=0.2, random_state=42) # fitting with early stopping xgb_model = XGBSEDebiasedBCE() xgb_model.fit( X_train, y_train, validation_data=(X_valid, y_valid), early_stopping_rounds=10, verbose_eval=50 ) [0] validation-aft-nloglik:16.86713 Will train until validation-aft-nloglik hasn't improved in 10 rounds. [50] validation-aft-nloglik:3.64540 [100] validation-aft-nloglik:3.53679 [150] validation-aft-nloglik:3.53207 Stopping. Best iteration: [174] validation-aft-nloglik:3.53004 Explainability through prototypes \u00b6 xgbse also provides explainability through prototypes, searching the embedding for neighbors. The idea is to explain model predictions with real samples, providing solid ground to justify them (see [8]). The method .get_neighbors() searches for the n_neighbors nearest neighbors in index_data for each sample in query_data : neighbors = xgb_model.get_neighbors( query_data=X_valid, index_data=X_train, n_neighbors=10 ) neighbors.head(5) index neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 neighbor_6 neighbor_7 neighbor_8 neighbor_9 neighbor_10 1225 1151 1513 1200 146 215 452 1284 1127 1895 257 111 1897 1090 1743 1224 892 1695 1624 1546 1418 4 554 9 627 1257 1460 1031 1575 1557 440 1236 858 526 726 1042 177 1640 242 1529 234 1800 399 1431 1313 205 1738 599 954 1694 1715 1651 828 541 992 This way, we can inspect neighbors of a given sample to try to explain predictions. For instance, we can choose a reference and check that its neighbors actually are very similar as a sanity check: i = 0 reference = X_valid.iloc[i] reference.name = 'reference' train_neighs = X_train.loc[neighbors.iloc[i]] pd.concat([reference.to_frame().T, train_neighs]) index x0 x1 x2 x3 x4 x5 x6 x7 x8 reference 5.7 5.7 11 5.6 1 1 0 1 86 1151 5.8 5.9 11 5.5 1 1 0 1 82 1513 5.5 5.5 11 5.6 1 1 0 1 79 1200 5.7 6 11 5.6 1 1 0 1 76 146 5.9 5.9 11 5.5 0 1 0 1 75 215 5.8 5.5 11 5.4 1 1 0 1 78 452 5.7 5.7 12 5.5 0 0 0 1 76 1284 5.6 6.2 11 5.6 1 0 0 1 79 1127 5.5 5.1 11 5.5 1 1 0 1 86 1895 5.5 5.4 10 5.5 1 1 0 1 85 257 5.7 6 9.6 5.6 1 1 0 1 76 We also can compare the Kaplan-Meier curve estimated from the neighbors to the actual model prediction, checking that it is inside the confidence interval: from xgbse.non_parametric import calculate_kaplan_vectorized mean, high, low = calculate_kaplan_vectorized( np.array([y['c2'][neighbors.iloc[i]]]), np.array([y['c1'][neighbors.iloc[i]]]), TIME_BINS ) model_surv = xgb_model.predict(X_valid) plt.figure(figsize=(12,4), dpi=120) plt.plot(model_surv.columns, model_surv.iloc[i]) plt.plot(mean.columns, mean.iloc[0]) plt.fill_between(mean.columns, low.iloc[0], high.iloc[0], alpha=0.1, color='red') Specifically, for XBGSEKaplanNeighbors prototype predictions and model predictions should match exactly if n_neighbors is the same and query_data is equal to the training data. Metrics \u00b6 We made our own metrics submodule to make the lib self-contained. xgbse.metrics implements C-index, Brier Score and D-Calibration from [9], including adaptations to deal with censoring: # training model xgbse_model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, n_neighbors=30) xgbse_model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting preds = xgbse_model.predict(X_valid) # importing metrics from xgbse.metrics import ( concordance_index, approx_brier_score, dist_calibration_score ) # running metrics print(f'C-index: {concordance_index(y_valid, preds)}') print(f'Avg. Brier Score: {approx_brier_score(y_valid, preds)}') print(f\"\"\"D-Calibration: {dist_calibration_score(y_valid, preds) > 0.05}\"\"\") C-index: 0.6495863029409356 Avg. Brier Score: 0.1704190044350422 D-Calibration: True As metrics follow the score_func(y, y_pred, **kwargs) pattern, we can use the sklearn model selection module easily: from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer xgbse_model = XGBSEKaplanTree(PARAMS_TREE) results = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score)) results array([0.17432953, 0.15907712, 0.13783666, 0.16770409, 0.16792016]) References \u00b6 [1] Practical Lessons from Predicting Clicks on Ads at Facebook : paper that shows how stacking boosting models with logistic regression improves performance and calibration [2] Feature transformations with ensembles of trees : scikit-learn post showing tree ensembles as feature transformers [3] Calibration of probabilities for tree-based models : blog post showing a practical example of tree ensemble probability calibration with a logistic regression [4] Supervised dimensionality reduction and clustering at scale with RFs with UMAP : blog post showing how forests of decision trees act as noise filters, reducing intrinsic dimension of the dataset. [5] Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors : inspiration for the BCE method (multi-task logistic regression) [6] The Brier Score under Administrative Censoring: Problems and Solutions : reference to BCE (binary cross-entropy survival method). [7] The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis : reference we used for the Exponential Greenwood formula from KM confidence intervals [8] Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable : paper showing a very similar method for extracting prototypes [9] Effective Ways to Build and Evaluate Individual Survival Distributions : paper showing how to validate survival analysis models with different metrics Citing xgbse \u00b6 To cite this repository: @software{xgbse2020github, author = {Davi Vieira and Gabriel Gimenez and Guilherme Marmerola and Vitor Estima}, title = {XGBoost Survival Embeddings: improving statistical properties of XGBoost survival analysis implementation}, url = {http://github.com/loft-br/xgboost-survival-embeddings}, version = {0.2.0}, year = {2020}, }","title":"Basic Usage"},{"location":"basic-usage.html#api","text":"The package follows scikit-learn API, with a minor adaptation to work with time and event data ( y as a numpy structured array of times and events). .predict() returns a dataframe where each column is a time window and values represent the probability of survival before or exactly at the time window. # importing dataset from pycox package from pycox.datasets import metabric # importing model and utils from xgbse from xgbse import XGBSEKaplanNeighbors from xgbse.converters import convert_to_structured # getting data df = metabric.read_df() # splitting to X, y format X = df.drop(['duration', 'event'], axis=1) y = convert_to_structured(df['duration'], df['event']) # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors(n_neighbors=50) xgbse_model.fit(X, y) # predicting event_probs = xgbse_model.predict(X) event_probs.head() index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.98 0.87 0.81 0.74 0.71 0.66 0.53 0.47 0.42 0.4 0.3 0.25 0.21 0.16 0.12 0.098 0.085 0.062 0.054 1 0.99 0.89 0.79 0.7 0.64 0.58 0.53 0.46 0.42 0.39 0.33 0.31 0.3 0.24 0.21 0.18 0.16 0.11 0.095 2 0.94 0.78 0.63 0.57 0.54 0.49 0.44 0.37 0.34 0.32 0.26 0.23 0.21 0.16 0.13 0.11 0.098 0.072 0.062 3 0.99 0.95 0.93 0.88 0.84 0.81 0.73 0.67 0.57 0.52 0.45 0.37 0.33 0.28 0.23 0.19 0.16 0.12 0.1 4 0.98 0.92 0.82 0.77 0.72 0.68 0.63 0.6 0.57 0.55 0.51 0.48 0.45 0.42 0.38 0.33 0.3 0.22 0.2 You can also get interval predictions (probability of failing exactly at each time window) using return_interval_probs : # point predictions interval_probs = xgbse_model.predict(X_valid, return_interval_probs=True) interval_probs.head() index 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 0 0.024 0.1 0.058 0.07 0.034 0.05 0.12 0.061 0.049 0.026 0.096 0.049 0.039 0.056 0.04 0.02 0.013 0.023 0.0078 1 0.014 0.097 0.098 0.093 0.052 0.065 0.054 0.068 0.034 0.038 0.053 0.019 0.018 0.052 0.038 0.027 0.018 0.05 0.015 2 0.06 0.16 0.15 0.054 0.033 0.053 0.046 0.073 0.032 0.014 0.06 0.03 0.017 0.055 0.031 0.016 0.014 0.027 0.0097 3 0.011 0.034 0.021 0.053 0.038 0.038 0.08 0.052 0.1 0.049 0.075 0.079 0.037 0.052 0.053 0.041 0.026 0.04 0.017 4 0.016 0.067 0.099 0.046 0.05 0.042 0.051 0.028 0.03 0.018 0.048 0.022 0.029 0.038 0.035 0.047 0.031 0.08 0.027","title":"API"},{"location":"basic-usage.html#survival-curves-and-confidence-intervals","text":"XBGSEKaplanTree and XBGSEKaplanNeighbors support estimation of survival curves and confidence intervals via the Exponential Greenwood formula out-of-the-box via the return_ci argument: # fitting xgbse model xgbse_model = XGBSEKaplanNeighbors(n_neighbors=50) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) XGBSEDebiasedBCE does not support estimation of confidence intervals out-of-the-box, but we provide the XGBSEBootstrapEstimator to get non-parametric confidence intervals. As the stacked logistic regressions are trained with more samples (in comparison to neighbor-sets in XGBSEKaplanNeighbors ), confidence intervals are more concentrated: # base model as BCE base_model = XGBSEDebiasedBCE(PARAMS_XGB_AFT, PARAMS_LR) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=20) # fitting the meta estimator bootstrap_estimator.fit( X_train, y_train, validation_data=(X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS, ) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) The bootstrap abstraction can be used for XBGSEKaplanTree and XBGSEKaplanNeighbors as well, however, the confidence interval will be estimated via bootstrap only (not Exponential Greenwood formula): # base model base_model = XGBSEKaplanTree(PARAMS_TREE) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100) # fitting the meta estimator bootstrap_estimator.fit( X_train, y_train, time_bins=TIME_BINS, ) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) With a sufficiently large n_estimators , interval width shouldn't be much different, with the added benefit of model stability and improved accuracy. Addittionaly, XGBSEBootstrapEstimator allows building confidence intervals for interval probabilities (which is not supported for Exponential Greenwood): # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict( X_valid, return_ci=True, return_interval_probs=True ) # plotting CIs plot_ci(mean, upper_ci, lower_ci) The parameter ci_width controls the width of the confidence interval. For XGBSEKaplanTree it should be passed at .fit() , as KM curves are pre-calculated for each leaf at fit time to avoid storing training data. # fitting xgbse model xgbse_model = XGBSEKaplanTree(PARAMS_TREE) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS, ci_width=0.99) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_valid, return_ci=True) # plotting CIs plot_ci(mean, upper_ci, lower_ci) For other models ( XGBSEKaplanNeighbors and XGBSEBootstrapEstimator ) it should be passed at .predict() . # base model model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, N_NEIGHBORS) # fitting the meta estimator model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting mean, upper_ci, lower_ci = model.predict(X_valid, return_ci=True, ci_width=0.99) # plotting CIs plot_ci(mean, upper_ci, lower_ci)","title":"Survival curves and confidence intervals"},{"location":"basic-usage.html#early-stopping","text":"A simple interface to xgboost early stopping is provided. # splitting between train, and validation (X_train, X_valid, y_train, y_valid) = \\ train_test_split(X, y, test_size=0.2, random_state=42) # fitting with early stopping xgb_model = XGBSEDebiasedBCE() xgb_model.fit( X_train, y_train, validation_data=(X_valid, y_valid), early_stopping_rounds=10, verbose_eval=50 ) [0] validation-aft-nloglik:16.86713 Will train until validation-aft-nloglik hasn't improved in 10 rounds. [50] validation-aft-nloglik:3.64540 [100] validation-aft-nloglik:3.53679 [150] validation-aft-nloglik:3.53207 Stopping. Best iteration: [174] validation-aft-nloglik:3.53004","title":"Early stopping"},{"location":"basic-usage.html#explainability-through-prototypes","text":"xgbse also provides explainability through prototypes, searching the embedding for neighbors. The idea is to explain model predictions with real samples, providing solid ground to justify them (see [8]). The method .get_neighbors() searches for the n_neighbors nearest neighbors in index_data for each sample in query_data : neighbors = xgb_model.get_neighbors( query_data=X_valid, index_data=X_train, n_neighbors=10 ) neighbors.head(5) index neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 neighbor_6 neighbor_7 neighbor_8 neighbor_9 neighbor_10 1225 1151 1513 1200 146 215 452 1284 1127 1895 257 111 1897 1090 1743 1224 892 1695 1624 1546 1418 4 554 9 627 1257 1460 1031 1575 1557 440 1236 858 526 726 1042 177 1640 242 1529 234 1800 399 1431 1313 205 1738 599 954 1694 1715 1651 828 541 992 This way, we can inspect neighbors of a given sample to try to explain predictions. For instance, we can choose a reference and check that its neighbors actually are very similar as a sanity check: i = 0 reference = X_valid.iloc[i] reference.name = 'reference' train_neighs = X_train.loc[neighbors.iloc[i]] pd.concat([reference.to_frame().T, train_neighs]) index x0 x1 x2 x3 x4 x5 x6 x7 x8 reference 5.7 5.7 11 5.6 1 1 0 1 86 1151 5.8 5.9 11 5.5 1 1 0 1 82 1513 5.5 5.5 11 5.6 1 1 0 1 79 1200 5.7 6 11 5.6 1 1 0 1 76 146 5.9 5.9 11 5.5 0 1 0 1 75 215 5.8 5.5 11 5.4 1 1 0 1 78 452 5.7 5.7 12 5.5 0 0 0 1 76 1284 5.6 6.2 11 5.6 1 0 0 1 79 1127 5.5 5.1 11 5.5 1 1 0 1 86 1895 5.5 5.4 10 5.5 1 1 0 1 85 257 5.7 6 9.6 5.6 1 1 0 1 76 We also can compare the Kaplan-Meier curve estimated from the neighbors to the actual model prediction, checking that it is inside the confidence interval: from xgbse.non_parametric import calculate_kaplan_vectorized mean, high, low = calculate_kaplan_vectorized( np.array([y['c2'][neighbors.iloc[i]]]), np.array([y['c1'][neighbors.iloc[i]]]), TIME_BINS ) model_surv = xgb_model.predict(X_valid) plt.figure(figsize=(12,4), dpi=120) plt.plot(model_surv.columns, model_surv.iloc[i]) plt.plot(mean.columns, mean.iloc[0]) plt.fill_between(mean.columns, low.iloc[0], high.iloc[0], alpha=0.1, color='red') Specifically, for XBGSEKaplanNeighbors prototype predictions and model predictions should match exactly if n_neighbors is the same and query_data is equal to the training data.","title":"Explainability through prototypes"},{"location":"basic-usage.html#metrics","text":"We made our own metrics submodule to make the lib self-contained. xgbse.metrics implements C-index, Brier Score and D-Calibration from [9], including adaptations to deal with censoring: # training model xgbse_model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, n_neighbors=30) xgbse_model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting preds = xgbse_model.predict(X_valid) # importing metrics from xgbse.metrics import ( concordance_index, approx_brier_score, dist_calibration_score ) # running metrics print(f'C-index: {concordance_index(y_valid, preds)}') print(f'Avg. Brier Score: {approx_brier_score(y_valid, preds)}') print(f\"\"\"D-Calibration: {dist_calibration_score(y_valid, preds) > 0.05}\"\"\") C-index: 0.6495863029409356 Avg. Brier Score: 0.1704190044350422 D-Calibration: True As metrics follow the score_func(y, y_pred, **kwargs) pattern, we can use the sklearn model selection module easily: from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer xgbse_model = XGBSEKaplanTree(PARAMS_TREE) results = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score)) results array([0.17432953, 0.15907712, 0.13783666, 0.16770409, 0.16792016])","title":"Metrics"},{"location":"basic-usage.html#references","text":"[1] Practical Lessons from Predicting Clicks on Ads at Facebook : paper that shows how stacking boosting models with logistic regression improves performance and calibration [2] Feature transformations with ensembles of trees : scikit-learn post showing tree ensembles as feature transformers [3] Calibration of probabilities for tree-based models : blog post showing a practical example of tree ensemble probability calibration with a logistic regression [4] Supervised dimensionality reduction and clustering at scale with RFs with UMAP : blog post showing how forests of decision trees act as noise filters, reducing intrinsic dimension of the dataset. [5] Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors : inspiration for the BCE method (multi-task logistic regression) [6] The Brier Score under Administrative Censoring: Problems and Solutions : reference to BCE (binary cross-entropy survival method). [7] The Greenwood and Exponential Greenwood Confidence Intervals in Survival Analysis : reference we used for the Exponential Greenwood formula from KM confidence intervals [8] Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable : paper showing a very similar method for extracting prototypes [9] Effective Ways to Build and Evaluate Individual Survival Distributions : paper showing how to validate survival analysis models with different metrics","title":"References"},{"location":"basic-usage.html#citing-xgbse","text":"To cite this repository: @software{xgbse2020github, author = {Davi Vieira and Gabriel Gimenez and Guilherme Marmerola and Vitor Estima}, title = {XGBoost Survival Embeddings: improving statistical properties of XGBoost survival analysis implementation}, url = {http://github.com/loft-br/xgboost-survival-embeddings}, version = {0.2.0}, year = {2020}, }","title":"Citing xgbse"},{"location":"how_xgbse_works.html","text":"In this section, we try to make a quick introduction to xgbse . Refer to this this Notebook for the full code and/or if you want a more practical introduction. What xgbse tries to solve \u00b6 The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index) while being lightning fast. However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern: prediction of survival curves rather than point estimates estimation of confidence intervals calibrated (unbiased) expected survival times Let us take the AFT implementation as an example. The model assumes an underlying distribution for times and events, controlled by the aft_loss_distribution and aft_loss_distribution_scale hyperparameters. By tweaking the aft_loss_distribution_scale hyperparameter we can build models with very different average predicted survival times, while maintaing ordering, with good C-index results: # loop to show different scale results for scale in [1.5, 1.0, 0.5]: # chaning parameter PARAMS_XGB_AFT['aft_loss_distribution_scale'] = scale # training model bst = xgb.train( PARAMS_XGB_AFT, dtrain, num_boost_round=1000, early_stopping_rounds=10, evals=[(dval, 'val')], verbose_eval=0 ) # predicting and evaluating preds = bst.predict(dval) cind = concordance_index_censored(y_valid['c1'], y_valid['c2'], 1-preds) print(f\"aft_loss_distribution_scale: {scale}\") print(f\"C-index: {cind[0]:.3f}\") print(f\"Average survival time: {preds.mean():.0f} days\") print(\"----\") aft_loss_distribution_scale: 1.5 C-index: 0.645 Average survival time: 203 days ---- aft_loss_distribution_scale: 1.0 C-index: 0.648 Average survival time: 165 days ---- aft_loss_distribution_scale: 0.5 C-index: 0.646 Average survival time: 125 days ---- If we plot the average predictions alongside a unbiased survival estimator such as the Kaplan Meier we can check that for each step of 0.5 in aft_loss_distribution_scale we move roughly one decile to the right in the curve. So what predictions should we trust? Such sensitivity to hyperparameters ( 0.003 C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production. Leveraging xgboost as a feature transformer \u00b6 Although in need of an extension for statistical rigor, xgboost is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it. Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as feature transformers of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a feature transformation (embedding) of the input data. This kind of tree ensemble embedding has very convenient properties: sparsity and high-dimensionality: trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a Logistic Regression trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3]) supervision: trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a K-Nearest Neighbor model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4]) We take advantage of these properties in different ways as we will show in the next subsections. XGBSEDebiasedBCE : logistic regressions, time windows, embedding as input \u00b6 Our first approach, XGBSEDebiasedBCE , takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3]. It consists of training a set of logistic regressions on top of the embedding produced by xgboost , each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the di/ni term (point probabilities) in the Kaplan-Meier formula and then use the KM estimator to get nearly unbiased survival curves. This way, we can get full survival curves from xgboost , and confidence intervals with minor adaptations (such as performing some rounds of bootstrap). Training and scoring of logistic regression models is efficient, being performed in parallel through joblib , so the model can scale to hundreds of thousands or millions of samples. XGBSEStackedWeibull : XGBoost as risk estimator, Weibull AFT for survival curve \u00b6 In XGBSEStackedWeibull , we perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable. Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves). As we're stacking XGBoost with a single, one-variable parametric model (as opposed to XGBSEDebiasedBCE ), the model can be much faster (especially in training). We also have better extrapolation capabilities, due to stronger assumptions about the shape of the survival curve. However, these stronger assumptions may not fit some datasets as well as other methods. XGBSEKaplanNeighbors : Kaplan-Meier on nearest neighbors \u00b6 As explained in the previous section, even though the embedding produced by xgboost is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to \"convert\" xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator. We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula. However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search). XGBSEKaplanTree : single tree, and Kaplan-Meier on its leaves \u00b6 As a simplification to XGBSEKaplanNeighbors , we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via xgboost and calculate KM curves at each of its leaves. It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide XGBSEBootstrapEstimator , a bootstrap abstraction where we can fit a forest of XGBSEKaplanTree 's to improve accuracy and reduce variance. Does it solve the problem? \u00b6 Now we return to the first example and check how XGBEmbedKaplanNeighbors performs: # loop to show different scale results for scale in [1.5, 1.0, 0.5]: # chaning parameter PARAMS_XGB_AFT['aft_loss_distribution_scale'] = scale # training model xgbse_model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, n_neighbors=30) xgbse_model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting and evaluating preds = xgbse_model.predict(X_valid) cind = concordance_index_censored(y_valid['c1'], y_valid['c2'], (1-preds).mean(axis=1)) avg_probs = preds[[30, 90, 150]].mean().values.round(4).tolist() print(f\"aft_loss_distribution_scale: {scale}\") print(f\"C-index: {cind[0]:.3f}\") print(f\"Average probability of survival at [30, 90, 150] days: {avg_probs}\") print(\"----\") aft_loss_distribution_scale: 1.5 C-index: 0.640 Average probability of survival at [30, 90, 150] days: [0.9109, 0.6854, 0.528] ---- aft_loss_distribution_scale: 1.0 C-index: 0.644 Average probability of survival at [30, 90, 150] days: [0.9111, 0.6889, 0.5333] ---- aft_loss_distribution_scale: 0.5 C-index: 0.650 Average probability of survival at [30, 90, 150] days: [0.913, 0.6904, 0.5289] ---- As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of aft_loss_distribution_scale choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results: No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.","title":"How XGBSE works"},{"location":"how_xgbse_works.html#what-xgbse-tries-to-solve","text":"The XGBoost implementation provides two methods for survival analysis: Cox and Accelerated Failure Time (AFT). When it comes to ordering individuals by risk, both show competitive performance (as measured by C-index) while being lightning fast. However, we can observe shortcomings when it comes to other desirable statistical properties. Specifically, three properties are of concern: prediction of survival curves rather than point estimates estimation of confidence intervals calibrated (unbiased) expected survival times Let us take the AFT implementation as an example. The model assumes an underlying distribution for times and events, controlled by the aft_loss_distribution and aft_loss_distribution_scale hyperparameters. By tweaking the aft_loss_distribution_scale hyperparameter we can build models with very different average predicted survival times, while maintaing ordering, with good C-index results: # loop to show different scale results for scale in [1.5, 1.0, 0.5]: # chaning parameter PARAMS_XGB_AFT['aft_loss_distribution_scale'] = scale # training model bst = xgb.train( PARAMS_XGB_AFT, dtrain, num_boost_round=1000, early_stopping_rounds=10, evals=[(dval, 'val')], verbose_eval=0 ) # predicting and evaluating preds = bst.predict(dval) cind = concordance_index_censored(y_valid['c1'], y_valid['c2'], 1-preds) print(f\"aft_loss_distribution_scale: {scale}\") print(f\"C-index: {cind[0]:.3f}\") print(f\"Average survival time: {preds.mean():.0f} days\") print(\"----\") aft_loss_distribution_scale: 1.5 C-index: 0.645 Average survival time: 203 days ---- aft_loss_distribution_scale: 1.0 C-index: 0.648 Average survival time: 165 days ---- aft_loss_distribution_scale: 0.5 C-index: 0.646 Average survival time: 125 days ---- If we plot the average predictions alongside a unbiased survival estimator such as the Kaplan Meier we can check that for each step of 0.5 in aft_loss_distribution_scale we move roughly one decile to the right in the curve. So what predictions should we trust? Such sensitivity to hyperparameters ( 0.003 C-index variation yet 78 days difference) raises red flags for applications that are dependent on robust and calibrated time-to-event estimates, mining trust and preventing shipping survival analysis models to production.","title":"What xgbse tries to solve"},{"location":"how_xgbse_works.html#leveraging-xgboost-as-a-feature-transformer","text":"Although in need of an extension for statistical rigor, xgboost is still a powerhouse. C-index results show that the model can capture a great deal of signal, being competitive with the state of the art. We just need to adapt how we use it. Besides being leveraged for prediction tasks, Gradient Boosted Trees (GBTs) can also be used as feature transformers of the input data. Trees in the ensemble perform splits on features that discriminate the target, encoding the most relevant information for the task at hand in their structure. In particular, the terminal nodes (leaves) at each tree in the ensemble define a feature transformation (embedding) of the input data. This kind of tree ensemble embedding has very convenient properties: sparsity and high-dimensionality: trees deal with nonlinearity and cast original features to a sparse, high-dimensional embedding, which helps linear models perform well when trained on it. This allows a Logistic Regression trained on the embedding (as one-hot encoded leaf indices) to have comparable performance to the actual ensemble, with the added benefit of probability calibration (see [1], [2], and [3]) supervision: trees also work as a noise filter, performing splits only through features that have predictive power. Thus, the embedding actually has a lower intrinsic dimension than the input data. This mitigates the curse of dimensionality and allows a K-Nearest Neighbor model trained on the embedding (using hamming distance) to have comparable performance to the actual ensemble, with the added flexibility to apply any function over the neighbor-sets to get predictions. This arbitrary function can be, for instance, an unbiased survival estimator such as the Kaplan-Meier estimator (see [4]) We take advantage of these properties in different ways as we will show in the next subsections.","title":"Leveraging xgboost as a feature transformer"},{"location":"how_xgbse_works.html#xgbsedebiasedbce-logistic-regressions-time-windows-embedding-as-input","text":"Our first approach, XGBSEDebiasedBCE , takes inspiration from the multi-task logistic regression method in [5], the BCE approach in [6], and the probability calibration ideas from [1], [2] and [3]. It consists of training a set of logistic regressions on top of the embedding produced by xgboost , each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. The naive approach tends to give biased survival curves, due to the removal of censored individuals. Thus, we made some adaptations such that logistic regressions estimate the di/ni term (point probabilities) in the Kaplan-Meier formula and then use the KM estimator to get nearly unbiased survival curves. This way, we can get full survival curves from xgboost , and confidence intervals with minor adaptations (such as performing some rounds of bootstrap). Training and scoring of logistic regression models is efficient, being performed in parallel through joblib , so the model can scale to hundreds of thousands or millions of samples.","title":"XGBSEDebiasedBCE: logistic regressions, time windows, embedding as input"},{"location":"how_xgbse_works.html#xgbsestackedweibull-xgboost-as-risk-estimator-weibull-aft-for-survival-curve","text":"In XGBSEStackedWeibull , we perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable. Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves). As we're stacking XGBoost with a single, one-variable parametric model (as opposed to XGBSEDebiasedBCE ), the model can be much faster (especially in training). We also have better extrapolation capabilities, due to stronger assumptions about the shape of the survival curve. However, these stronger assumptions may not fit some datasets as well as other methods.","title":"XGBSEStackedWeibull: XGBoost as risk estimator, Weibull AFT for survival curve"},{"location":"how_xgbse_works.html#xgbsekaplanneighbors-kaplan-meier-on-nearest-neighbors","text":"As explained in the previous section, even though the embedding produced by xgboost is sparse and high dimensional, its intrisic dimensionality actually should be lower than the input data. This enables us to \"convert\" xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-occurred the most at the ensemble terminal nodes. Then, at each neighbor-set we can get survival estimates with robust methods such as the Kaplan-Meier estimator. We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. We built a high-performing implementation of the KM estimator to calculate several survival curves in a vectorized fashion, including upper and lower confidence intervals based on the Exponential Greenwood formula. However, this method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search).","title":"XGBSEKaplanNeighbors: Kaplan-Meier on nearest neighbors"},{"location":"how_xgbse_works.html#xgbsekaplantree-single-tree-and-kaplan-meier-on-its-leaves","text":"As a simplification to XGBSEKaplanNeighbors , we also provide a single tree implementation. Instead of doing expensive nearest neighbor searches, we fit a single tree via xgboost and calculate KM curves at each of its leaves. It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. However, as we're fitting a single tree, predictive power may be worse. That could be a sensible tradeoff, but we also provide XGBSEBootstrapEstimator , a bootstrap abstraction where we can fit a forest of XGBSEKaplanTree 's to improve accuracy and reduce variance.","title":"XGBSEKaplanTree: single tree, and Kaplan-Meier on its leaves"},{"location":"how_xgbse_works.html#does-it-solve-the-problem","text":"Now we return to the first example and check how XGBEmbedKaplanNeighbors performs: # loop to show different scale results for scale in [1.5, 1.0, 0.5]: # chaning parameter PARAMS_XGB_AFT['aft_loss_distribution_scale'] = scale # training model xgbse_model = XGBSEKaplanNeighbors(PARAMS_XGB_AFT, n_neighbors=30) xgbse_model.fit( X_train, y_train, validation_data = (X_valid, y_valid), early_stopping_rounds=10, time_bins=TIME_BINS ) # predicting and evaluating preds = xgbse_model.predict(X_valid) cind = concordance_index_censored(y_valid['c1'], y_valid['c2'], (1-preds).mean(axis=1)) avg_probs = preds[[30, 90, 150]].mean().values.round(4).tolist() print(f\"aft_loss_distribution_scale: {scale}\") print(f\"C-index: {cind[0]:.3f}\") print(f\"Average probability of survival at [30, 90, 150] days: {avg_probs}\") print(\"----\") aft_loss_distribution_scale: 1.5 C-index: 0.640 Average probability of survival at [30, 90, 150] days: [0.9109, 0.6854, 0.528] ---- aft_loss_distribution_scale: 1.0 C-index: 0.644 Average probability of survival at [30, 90, 150] days: [0.9111, 0.6889, 0.5333] ---- aft_loss_distribution_scale: 0.5 C-index: 0.650 Average probability of survival at [30, 90, 150] days: [0.913, 0.6904, 0.5289] ---- As measured by the average probability of survival in 30, 90 and 150 days the model is very stable, showing similar calibration results independently of aft_loss_distribution_scale choice, with comparable (or a bit worse) C-index results. Visually, the comparison of the average model predictions to a Kaplan Meier yields much better results: No more point estimates and high variation! Although is too harsh to claim that the problem is solved, we believe that the package can be a good, more statistically robust alternative to survival analysis.","title":"Does it solve the problem?"},{"location":"install.html","text":"You can easily install xgbse via pip: pip install xgbse","title":"Install"},{"location":"benchmarks/benchmarks.html","text":"Metrics \u00b6 In the examples folder you'll find benchmarks comparing xgbse to other survival analysis methods. We show 6 metrics (see [9] for details): c-index : concordance index. Equivalent to AUC with censored data. dcal_max_dev : maximum decile deviation from calibrated distribution. dcal_pval : p-value from chi-square test checking for D-Calibration. If larger than 0.05 then the model is D-Calibrated. ibs : approximate integrated brier score, the average brier score across all time windows. inference_time : time to perform inference, recorded on a 2018 MacBook Pro. training_time : time to perform training, recorded on a 2018 MacBook Pro. We executed all methods with default parameters. For vanilla XGBoost and xgbse , early stopping was used, with num_boosting_rounds=1000 , and early_stopping_rounds=10 . We show results below for five datasets. Results \u00b6 FLCHAIN \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time Weibull AFT 0.789 0.013 0.849 0.099 0.006 0.537 Cox-PH 0.788 0.011 0.971 0.099 0.005 0.942 XGBSE - Debiased BCE 0.784 0.037 0 0.117 0.233 3.062 XGBSE - Bootstrap Trees 0.781 0.009 0.985 0.1 0.382 15.351 XGBSE - Kaplan Neighbors 0.777 0.013 0.918 0.102 0.543 0.479 XGBSE - Stacked Weibull 0.776 0.008 0.994 0.103 0.011 0.719 XGB - Cox 0.775 nan nan nan 0.001 0.054 XGB - AFT 0.772 nan nan nan 0.001 0.106 XGBSE - Kaplan Tree 0.768 0.011 0.929 0.103 0.003 0.167 METABRIC \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.63 0.045 0.146 0.162 0.01 0.525 XGBSE - Debiased BCE 0.627 0.033 0.128 0.165 0.09 3.165 XGBSE - Bootstrap Trees 0.624 0.024 0.563 0.155 0.301 6.165 Weibull AFT 0.622 0.024 0.667 0.154 0.005 0.284 Cox-PH 0.622 0.026 0.567 0.154 0.004 0.244 XGB - Cox 0.617 nan nan nan 0.001 0.096 XGBSE - Kaplan Neighbors 0.605 0.023 0.588 0.163 0.111 0.154 XGB - AFT 0.6 nan nan nan 0.001 0.044 XGBSE - Kaplan Tree 0.59 0.036 0.18 0.165 0.002 0.05 RRNLNPH \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.826 0.05 0 0.113 0.019 2.255 XGBSE - Bootstrap Trees 0.826 0.035 0 0.097 0.534 44.736 XGBSE - Kaplan Neighbors 0.824 0.038 0 0.1 15.662 1.504 XGBSE - Debiased BCE 0.824 0.068 0 0.108 0.285 4.562 XGB - Cox 0.824 nan nan nan 0.002 0.375 XGB - AFT 0.823 nan nan nan 0.001 0.243 XGBSE - Kaplan Tree 0.821 0.044 0 0.101 0.006 0.49 Weibull AFT 0.787 0.057 0 0.136 0.01 0.326 Cox-PH 0.787 0.055 0 0.135 0.021 2.267 SAC3 \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.697 0.04 0 0.171 0.153 32.469 XGB - AFT 0.691 nan nan nan 0.004 7.413 XGBSE - Debiased BCE 0.69 0.045 0 0.169 1.141 47.814 XGB - Cox 0.686 nan nan nan 0.002 4.885 Cox-PH 0.682 0.035 0 0.165 0.039 1.84 Weibull AFT 0.682 0.039 0 0.165 0.043 2.307 XGBSE - Bootstrap Trees 0.677 0.043 0 0.168 3.134 164.173 XGBSE - Kaplan Neighbors 0.666 0.037 0 0.175 416.382 36.759 XGBSE - Kaplan Tree 0.631 0.034 0 0.191 0.036 1.478 SUPPORT \u00b6 model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.621 0.092 0 0.198 0.013 0.954 XGBSE - Debiased BCE 0.617 0.139 0 0.188 0.272 3.852 XGB - Cox 0.61 nan nan nan 0.001 0.069 XGB - AFT 0.609 nan nan nan 0.001 0.137 XGBSE - Bootstrap Trees 0.607 0.103 0 0.188 0.371 18.202 XGBSE - Kaplan Neighbors 0.601 0.099 0 0.197 1.488 0.752 XGBSE - Kaplan Tree 0.598 0.097 0 0.203 0.004 0.149 Cox-PH 0.578 0.16 0 0.201 0.006 0.465 Weibull AFT 0.576 0.138 0 0.201 0.007 0.461 Analysis \u00b6 XGBSEDebiasedBCE and XGBSEStackedWeibull show the most promising results, being in the top three methods 4 out of 5 times. Other xgbse methods show good results too. In particular XGBSEKaplanTree with XGBSEBootstrapEstimator shows promising results, pointing to a direction for further research. Linear methods such as the Weibull AFT and Cox-PH from lifelines are surprisingly strong, specially for datasets with a small number of samples. xgbse methods show competitive results to vanilla xgboost as measured by C-index, while showing good results for \"survival curve metrics\". Thus, we can use xgbse as a calibrated replacement to vanilla xgboost . xgbse takes longer to fit than vanilla xgboost . Specially for XGBSEDebiasedBCE , we have to build N logistic regressions where N is the number of time windows we'll predict. In all cases we used N = 30. XGBSEStackedWeibull is the most efficient method, behind XGBSEKaplanTree .","title":"Benchmarks"},{"location":"benchmarks/benchmarks.html#metrics","text":"In the examples folder you'll find benchmarks comparing xgbse to other survival analysis methods. We show 6 metrics (see [9] for details): c-index : concordance index. Equivalent to AUC with censored data. dcal_max_dev : maximum decile deviation from calibrated distribution. dcal_pval : p-value from chi-square test checking for D-Calibration. If larger than 0.05 then the model is D-Calibrated. ibs : approximate integrated brier score, the average brier score across all time windows. inference_time : time to perform inference, recorded on a 2018 MacBook Pro. training_time : time to perform training, recorded on a 2018 MacBook Pro. We executed all methods with default parameters. For vanilla XGBoost and xgbse , early stopping was used, with num_boosting_rounds=1000 , and early_stopping_rounds=10 . We show results below for five datasets.","title":"Metrics"},{"location":"benchmarks/benchmarks.html#results","text":"","title":"Results"},{"location":"benchmarks/benchmarks.html#flchain","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time Weibull AFT 0.789 0.013 0.849 0.099 0.006 0.537 Cox-PH 0.788 0.011 0.971 0.099 0.005 0.942 XGBSE - Debiased BCE 0.784 0.037 0 0.117 0.233 3.062 XGBSE - Bootstrap Trees 0.781 0.009 0.985 0.1 0.382 15.351 XGBSE - Kaplan Neighbors 0.777 0.013 0.918 0.102 0.543 0.479 XGBSE - Stacked Weibull 0.776 0.008 0.994 0.103 0.011 0.719 XGB - Cox 0.775 nan nan nan 0.001 0.054 XGB - AFT 0.772 nan nan nan 0.001 0.106 XGBSE - Kaplan Tree 0.768 0.011 0.929 0.103 0.003 0.167","title":"FLCHAIN"},{"location":"benchmarks/benchmarks.html#metabric","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.63 0.045 0.146 0.162 0.01 0.525 XGBSE - Debiased BCE 0.627 0.033 0.128 0.165 0.09 3.165 XGBSE - Bootstrap Trees 0.624 0.024 0.563 0.155 0.301 6.165 Weibull AFT 0.622 0.024 0.667 0.154 0.005 0.284 Cox-PH 0.622 0.026 0.567 0.154 0.004 0.244 XGB - Cox 0.617 nan nan nan 0.001 0.096 XGBSE - Kaplan Neighbors 0.605 0.023 0.588 0.163 0.111 0.154 XGB - AFT 0.6 nan nan nan 0.001 0.044 XGBSE - Kaplan Tree 0.59 0.036 0.18 0.165 0.002 0.05","title":"METABRIC"},{"location":"benchmarks/benchmarks.html#rrnlnph","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.826 0.05 0 0.113 0.019 2.255 XGBSE - Bootstrap Trees 0.826 0.035 0 0.097 0.534 44.736 XGBSE - Kaplan Neighbors 0.824 0.038 0 0.1 15.662 1.504 XGBSE - Debiased BCE 0.824 0.068 0 0.108 0.285 4.562 XGB - Cox 0.824 nan nan nan 0.002 0.375 XGB - AFT 0.823 nan nan nan 0.001 0.243 XGBSE - Kaplan Tree 0.821 0.044 0 0.101 0.006 0.49 Weibull AFT 0.787 0.057 0 0.136 0.01 0.326 Cox-PH 0.787 0.055 0 0.135 0.021 2.267","title":"RRNLNPH"},{"location":"benchmarks/benchmarks.html#sac3","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.697 0.04 0 0.171 0.153 32.469 XGB - AFT 0.691 nan nan nan 0.004 7.413 XGBSE - Debiased BCE 0.69 0.045 0 0.169 1.141 47.814 XGB - Cox 0.686 nan nan nan 0.002 4.885 Cox-PH 0.682 0.035 0 0.165 0.039 1.84 Weibull AFT 0.682 0.039 0 0.165 0.043 2.307 XGBSE - Bootstrap Trees 0.677 0.043 0 0.168 3.134 164.173 XGBSE - Kaplan Neighbors 0.666 0.037 0 0.175 416.382 36.759 XGBSE - Kaplan Tree 0.631 0.034 0 0.191 0.036 1.478","title":"SAC3"},{"location":"benchmarks/benchmarks.html#support","text":"model c-index dcal_max_dev dcal_pval ibs inference_time training_time XGBSE - Stacked Weibull 0.621 0.092 0 0.198 0.013 0.954 XGBSE - Debiased BCE 0.617 0.139 0 0.188 0.272 3.852 XGB - Cox 0.61 nan nan nan 0.001 0.069 XGB - AFT 0.609 nan nan nan 0.001 0.137 XGBSE - Bootstrap Trees 0.607 0.103 0 0.188 0.371 18.202 XGBSE - Kaplan Neighbors 0.601 0.099 0 0.197 1.488 0.752 XGBSE - Kaplan Tree 0.598 0.097 0 0.203 0.004 0.149 Cox-PH 0.578 0.16 0 0.201 0.006 0.465 Weibull AFT 0.576 0.138 0 0.201 0.007 0.461","title":"SUPPORT"},{"location":"benchmarks/benchmarks.html#analysis","text":"XGBSEDebiasedBCE and XGBSEStackedWeibull show the most promising results, being in the top three methods 4 out of 5 times. Other xgbse methods show good results too. In particular XGBSEKaplanTree with XGBSEBootstrapEstimator shows promising results, pointing to a direction for further research. Linear methods such as the Weibull AFT and Cox-PH from lifelines are surprisingly strong, specially for datasets with a small number of samples. xgbse methods show competitive results to vanilla xgboost as measured by C-index, while showing good results for \"survival curve metrics\". Thus, we can use xgbse as a calibrated replacement to vanilla xgboost . xgbse takes longer to fit than vanilla xgboost . Specially for XGBSEDebiasedBCE , we have to build N logistic regressions where N is the number of time windows we'll predict. In all cases we used N = 30. XGBSEStackedWeibull is the most efficient method, behind XGBSEKaplanTree .","title":"Analysis"},{"location":"examples/basic_usage.html","text":"Basic Usage: \u00b6 In this notebook you will find: - How to get a survival curve and neighbors prediction using xgbse - How to validate your xgbse model using sklearn Metrabic \u00b6 We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1 Split and Time Bins \u00b6 Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]) Fit and Predict \u00b6 We will be using the DebiasedBCE estimator to fit the model and predict a survival curve for each point in our test data from xgbse import XGBSEDebiasedBCE # fitting xgbse model xgbse_model = XGBSEDebiasedBCE() xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting y_pred = xgbse_model.predict(X_test) print(y_pred.shape) y_pred.head() (635, 20) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566 mean predicted survival curve for test data y_pred.mean().plot.line(); Neighbors \u00b6 We can also use our model for querying comparables based on survivability. neighbors = xgbse_model.get_neighbors( query_data = X_test, index_data = X_train, n_neighbors = 5 ) print(neighbors.shape) neighbors.head(5) (635, 5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 829 339 166 508 1879 418 670 1846 1082 1297 194 1448 1064 416 1230 739 1392 589 85 1558 8 1080 613 1522 1814 105 859 1743 50 566 example : selecting a data point from query data (X_test) and checking its features desired = neighbors.iloc[10] X_test.loc[X_test.index == desired.name] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 399 5.572504 7.367552 11.023443 5.406307 1.0 0.0 0.0 1.0 67.620003 ... and finding its comparables from index data (X_train) X_train.loc[X_train.index.isin(desired.tolist())] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 757 5.745395 8.178815 10.745699 5.530381 1.0 1.0 0.0 1.0 64.930000 726 5.635854 6.648942 10.889588 5.496374 1.0 1.0 0.0 1.0 70.860001 968 5.541239 7.058089 10.463409 5.396433 1.0 0.0 0.0 1.0 71.070000 870 5.605712 7.309217 10.935708 5.542732 0.0 1.0 0.0 1.0 71.470001 1640 5.812605 7.646811 10.952687 5.516386 1.0 1.0 0.0 1.0 68.559998 Score metrics \u00b6 XGBSE implements concordance index and integrated brier score, both can be used to evaluate model performance # importing metrics from xgbse.metrics import concordance_index, approx_brier_score # running metrics print(f\"C-index: {concordance_index(y_test, y_pred)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, y_pred)}\") C-index: 0.6706453426714781 Avg. Brier Score: 0.17221909077845754 Cross Validation \u00b6 We can also use sklearn's cross_val_score and make_scorer to cross validate our model from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer results = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score)) results array([0.16269636, 0.14880423, 0.12848939, 0.15335356, 0.15394174])","title":"Basic Usage:"},{"location":"examples/basic_usage.html#basic-usage","text":"In this notebook you will find: - How to get a survival curve and neighbors prediction using xgbse - How to validate your xgbse model using sklearn","title":"Basic Usage:"},{"location":"examples/basic_usage.html#metrabic","text":"We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1","title":"Metrabic"},{"location":"examples/basic_usage.html#split-and-time-bins","text":"Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300])","title":"Split and Time Bins"},{"location":"examples/basic_usage.html#fit-and-predict","text":"We will be using the DebiasedBCE estimator to fit the model and predict a survival curve for each point in our test data from xgbse import XGBSEDebiasedBCE # fitting xgbse model xgbse_model = XGBSEDebiasedBCE() xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting y_pred = xgbse_model.predict(X_test) print(y_pred.shape) y_pred.head() (635, 20) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566 mean predicted survival curve for test data y_pred.mean().plot.line();","title":"Fit and Predict"},{"location":"examples/basic_usage.html#neighbors","text":"We can also use our model for querying comparables based on survivability. neighbors = xgbse_model.get_neighbors( query_data = X_test, index_data = X_train, n_neighbors = 5 ) print(neighbors.shape) neighbors.head(5) (635, 5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } neighbor_1 neighbor_2 neighbor_3 neighbor_4 neighbor_5 829 339 166 508 1879 418 670 1846 1082 1297 194 1448 1064 416 1230 739 1392 589 85 1558 8 1080 613 1522 1814 105 859 1743 50 566 example : selecting a data point from query data (X_test) and checking its features desired = neighbors.iloc[10] X_test.loc[X_test.index == desired.name] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 399 5.572504 7.367552 11.023443 5.406307 1.0 0.0 0.0 1.0 67.620003 ... and finding its comparables from index data (X_train) X_train.loc[X_train.index.isin(desired.tolist())] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 757 5.745395 8.178815 10.745699 5.530381 1.0 1.0 0.0 1.0 64.930000 726 5.635854 6.648942 10.889588 5.496374 1.0 1.0 0.0 1.0 70.860001 968 5.541239 7.058089 10.463409 5.396433 1.0 0.0 0.0 1.0 71.070000 870 5.605712 7.309217 10.935708 5.542732 0.0 1.0 0.0 1.0 71.470001 1640 5.812605 7.646811 10.952687 5.516386 1.0 1.0 0.0 1.0 68.559998","title":"Neighbors"},{"location":"examples/basic_usage.html#score-metrics","text":"XGBSE implements concordance index and integrated brier score, both can be used to evaluate model performance # importing metrics from xgbse.metrics import concordance_index, approx_brier_score # running metrics print(f\"C-index: {concordance_index(y_test, y_pred)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, y_pred)}\") C-index: 0.6706453426714781 Avg. Brier Score: 0.17221909077845754","title":"Score metrics"},{"location":"examples/basic_usage.html#cross-validation","text":"We can also use sklearn's cross_val_score and make_scorer to cross validate our model from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer results = cross_val_score(xgbse_model, X, y, scoring=make_scorer(approx_brier_score)) results array([0.16269636, 0.14880423, 0.12848939, 0.15335356, 0.15394174])","title":"Cross Validation"},{"location":"examples/confidence_interval.html","text":"Confidence Interval: \u00b6 In this notebook you will find: - Get confidence intervals for predicted survival curves using XGBSE estimators; - How to use XGBSEBootstrapEstimator, a meta estimator for bagging; - A nice function to help us plot survival curves. import matplotlib.pyplot as plt plt.style.use('bmh') from IPython.display import set_matplotlib_formats set_matplotlib_formats('retina') # to easily plot confidence intervals def plot_ci(mean, upper_ci, lower_ci, i=42, title='Probability of survival $P(T \\geq t)$'): # plotting mean and confidence intervals plt.figure(figsize=(12, 4), dpi=120) plt.plot(mean.columns,mean.iloc[i]) plt.fill_between(mean.columns, lower_ci.iloc[i], upper_ci.iloc[i], alpha=0.2) plt.title(title) plt.xlabel('Time [days]') plt.ylabel('Probability') plt.tight_layout() Metrabic \u00b6 We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1 Split and Time Bins \u00b6 Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve. from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]) Calculating confidence intervals \u00b6 We will be using the XGBSEKaplanTree estimator to fit the model and predict a survival curve for each point in our test data, and via return_ci parameter we will get upper and lower bounds for the confidence interval. from xgbse import XGBSEKaplanTree, XGBSEBootstrapEstimator from xgbse.metrics import concordance_index, approx_brier_score # xgboost parameters to fit our model PARAMS_TREE = { 'objective': 'survival:cox', 'eval_metric': 'cox-nloglik', 'tree_method': 'hist', 'max_depth': 10, 'booster':'dart', 'subsample': 1.0, 'min_child_weight': 50, 'colsample_bynode': 1.0 } Numerical Form \u00b6 The KaplanTree and KaplanNeighbors models support estimation of confidence intervals via the Exponential Greenwood formula. %%time # fitting xgbse model xgbse_model = XGBSEKaplanTree(PARAMS_TREE) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_test, return_ci=True) # print metrics print(f\"C-index: {concordance_index(y_test, mean)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\") # plotting CIs plot_ci(mean, upper_ci, lower_ci) C-index: 0.6358942056527093 Avg. Brier Score: 0.182841148106733 CPU times: user 1.88 s, sys: 9.37 ms, total: 1.88 s Wall time: 897 ms Non-parametric Form \u00b6 We can also use the XGBSEBootstrapEstimator to wrap any XGBSE model and get confidence intervals via bagging, which also slighty increase our performance at the cost of computation time. %%time # base model as XGBSEKaplanTree base_model = XGBSEKaplanTree(PARAMS_TREE) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100) # fitting the meta estimator bootstrap_estimator.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_test, return_ci=True) # print metrics print(f\"C-index: {concordance_index(y_test, mean)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\") # plotting CIs plot_ci(mean, upper_ci, lower_ci) C-index: 0.6580651819585904 Avg. Brier Score: 0.17040560738276272 CPU times: user 17.3 s, sys: 58.5 ms, total: 17.4 s Wall time: 4.18 s","title":"Confidence intervals"},{"location":"examples/confidence_interval.html#confidence-interval","text":"In this notebook you will find: - Get confidence intervals for predicted survival curves using XGBSE estimators; - How to use XGBSEBootstrapEstimator, a meta estimator for bagging; - A nice function to help us plot survival curves. import matplotlib.pyplot as plt plt.style.use('bmh') from IPython.display import set_matplotlib_formats set_matplotlib_formats('retina') # to easily plot confidence intervals def plot_ci(mean, upper_ci, lower_ci, i=42, title='Probability of survival $P(T \\geq t)$'): # plotting mean and confidence intervals plt.figure(figsize=(12, 4), dpi=120) plt.plot(mean.columns,mean.iloc[i]) plt.fill_between(mean.columns, lower_ci.iloc[i], upper_ci.iloc[i], alpha=0.2) plt.title(title) plt.xlabel('Time [days]') plt.ylabel('Probability') plt.tight_layout()","title":"Confidence Interval:"},{"location":"examples/confidence_interval.html#metrabic","text":"We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1","title":"Metrabic"},{"location":"examples/confidence_interval.html#split-and-time-bins","text":"Split the data in train and test, using sklearn API. We also setup the TIME_BINS array, which will be used to fit the survival curve. from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300])","title":"Split and Time Bins"},{"location":"examples/confidence_interval.html#calculating-confidence-intervals","text":"We will be using the XGBSEKaplanTree estimator to fit the model and predict a survival curve for each point in our test data, and via return_ci parameter we will get upper and lower bounds for the confidence interval. from xgbse import XGBSEKaplanTree, XGBSEBootstrapEstimator from xgbse.metrics import concordance_index, approx_brier_score # xgboost parameters to fit our model PARAMS_TREE = { 'objective': 'survival:cox', 'eval_metric': 'cox-nloglik', 'tree_method': 'hist', 'max_depth': 10, 'booster':'dart', 'subsample': 1.0, 'min_child_weight': 50, 'colsample_bynode': 1.0 }","title":"Calculating confidence intervals"},{"location":"examples/confidence_interval.html#numerical-form","text":"The KaplanTree and KaplanNeighbors models support estimation of confidence intervals via the Exponential Greenwood formula. %%time # fitting xgbse model xgbse_model = XGBSEKaplanTree(PARAMS_TREE) xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = xgbse_model.predict(X_test, return_ci=True) # print metrics print(f\"C-index: {concordance_index(y_test, mean)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\") # plotting CIs plot_ci(mean, upper_ci, lower_ci) C-index: 0.6358942056527093 Avg. Brier Score: 0.182841148106733 CPU times: user 1.88 s, sys: 9.37 ms, total: 1.88 s Wall time: 897 ms","title":"Numerical Form"},{"location":"examples/confidence_interval.html#non-parametric-form","text":"We can also use the XGBSEBootstrapEstimator to wrap any XGBSE model and get confidence intervals via bagging, which also slighty increase our performance at the cost of computation time. %%time # base model as XGBSEKaplanTree base_model = XGBSEKaplanTree(PARAMS_TREE) # bootstrap meta estimator bootstrap_estimator = XGBSEBootstrapEstimator(base_model, n_estimators=100) # fitting the meta estimator bootstrap_estimator.fit(X_train, y_train, time_bins=TIME_BINS) # predicting mean, upper_ci, lower_ci = bootstrap_estimator.predict(X_test, return_ci=True) # print metrics print(f\"C-index: {concordance_index(y_test, mean)}\") print(f\"Avg. Brier Score: {approx_brier_score(y_test, mean)}\") # plotting CIs plot_ci(mean, upper_ci, lower_ci) C-index: 0.6580651819585904 Avg. Brier Score: 0.17040560738276272 CPU times: user 17.3 s, sys: 58.5 ms, total: 17.4 s Wall time: 4.18 s","title":"Non-parametric Form"},{"location":"examples/extrapolation_example.html","text":"Extrapolation \u00b6 In this notebook you will find: - How to get a survival curve using xgbse - How to extrapolate your predicted survival curve using the xgbse.extrapolation module Metrabic \u00b6 We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1 Split and create Time Bins \u00b6 Split the data in train and test, using sklearn API. We also setup the TIME_BINS arange, which will be used to fit the survival curve from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]) Fit model and predict survival curves \u00b6 The package follows scikit-learn API, with a minor adaptation to work with time and event data. The model outputs the probability of survival, in a pd.Dataframe where columns represent different times. from xgbse import XGBSEDebiasedBCE # fitting xgbse model xgbse_model = XGBSEDebiasedBCE() xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting survival = xgbse_model.predict(X_test) survival.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566 Survival curves visualization \u00b6 import matplotlib.pyplot as plt plt.figure(figsize=(12,4), dpi=120) plt.plot( survival.columns, survival.iloc[42], 'k--', label='Survival' ) plt.title('Sample of predicted survival curves - $P(T>t)$') plt.legend() <matplotlib.legend.Legend at 0x7fc38026cef0> Notice that this predicted survival curve does not end at zero (cure fraction due to censored data). In some cases it might be useful to extrapolate our survival curves using specific strategies. xgbse.extrapolation implements a constant risk extrapolation strategy. Extrapolation \u00b6 from xgbse.extrapolation import extrapolate_constant_risk # extrapolating predicted survival survival_ext = extrapolate_constant_risk(survival, 450, 11) survival_ext.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15.0 30.0 45.0 60.0 75.0 90.0 105.0 120.0 135.0 150.0 ... 315.0 330.0 345.0 360.0 375.0 390.0 405.0 420.0 435.0 450.0 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 ... 0.112299 0.068213 0.032292 0.011915 0.003426 0.000768 0.000134 1.825794e-05 1.937124e-06 1.601799e-07 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 ... 0.026665 0.014641 0.005957 0.001796 0.000401 0.000066 0.000008 7.404100e-07 4.986652e-08 2.488634e-09 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 ... 0.088900 0.049982 0.021071 0.006660 0.001579 0.000281 0.000037 3.735612e-06 2.798762e-07 1.572266e-08 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 ... 0.067080 0.038160 0.016373 0.005299 0.001293 0.000238 0.000033 3.462388e-06 2.734946e-07 1.629408e-08 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 ... 0.050668 0.026879 0.010385 0.002923 0.000599 0.000089 0.000010 7.701555e-07 4.442463e-08 1.866412e-09 5 rows \u00d7 31 columns # plotting extrapolation # plt.figure(figsize=(12,4), dpi=120) plt.plot( survival.columns, survival.iloc[42], 'k--', label='Survival' ) plt.plot( survival_ext.columns, survival_ext.iloc[42], 'tomato', alpha=0.5, label='Extrapolated Survival' ) plt.title('Extrapolation of survival curves') plt.legend() <matplotlib.legend.Legend at 0x7fc3801842b0>","title":"Extrapolation"},{"location":"examples/extrapolation_example.html#extrapolation","text":"In this notebook you will find: - How to get a survival curve using xgbse - How to extrapolate your predicted survival curve using the xgbse.extrapolation module","title":"Extrapolation"},{"location":"examples/extrapolation_example.html#metrabic","text":"We will be using the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset from pycox as base for this example. from xgbse.converters import convert_to_structured from pycox.datasets import metabric import numpy as np # getting data df = metabric.read_df() df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x0 x1 x2 x3 x4 x5 x6 x7 x8 duration event 0 5.603834 7.811392 10.797988 5.967607 1.0 1.0 0.0 1.0 56.840000 99.333336 0 1 5.284882 9.581043 10.204620 5.664970 1.0 0.0 0.0 1.0 85.940002 95.733330 1 2 5.920251 6.776564 12.431715 5.873857 0.0 1.0 0.0 1.0 48.439999 140.233337 0 3 6.654017 5.341846 8.646379 5.655888 0.0 0.0 0.0 0.0 66.910004 239.300003 0 4 5.456747 5.339741 10.555724 6.008429 1.0 0.0 0.0 1.0 67.849998 56.933334 1","title":"Metrabic"},{"location":"examples/extrapolation_example.html#split-and-create-time-bins","text":"Split the data in train and test, using sklearn API. We also setup the TIME_BINS arange, which will be used to fit the survival curve from xgbse.converters import convert_to_structured from sklearn.model_selection import train_test_split # splitting to X, T, E format X = df.drop(['duration', 'event'], axis=1) T = df['duration'] E = df['event'] y = convert_to_structured(T, E) # splitting between train, and validation X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state = 0) TIME_BINS = np.arange(15, 315, 15) TIME_BINS array([ 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300])","title":"Split and create Time Bins"},{"location":"examples/extrapolation_example.html#fit-model-and-predict-survival-curves","text":"The package follows scikit-learn API, with a minor adaptation to work with time and event data. The model outputs the probability of survival, in a pd.Dataframe where columns represent different times. from xgbse import XGBSEDebiasedBCE # fitting xgbse model xgbse_model = XGBSEDebiasedBCE() xgbse_model.fit(X_train, y_train, time_bins=TIME_BINS) # predicting survival = xgbse_model.predict(X_test) survival.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 0.568001 0.513172 0.493194 0.430701 0.377675 0.310496 0.272169 0.225599 0.184878 0.144089 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 0.226226 0.191373 0.171697 0.144864 0.112447 0.089558 0.081137 0.057679 0.048563 0.035985 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 0.485275 0.451667 0.428899 0.386413 0.344369 0.279685 0.242064 0.187967 0.158121 0.118562 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 0.493957 0.443193 0.416702 0.376552 0.308947 0.237033 0.177140 0.141838 0.117917 0.088937 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 0.416363 0.391099 0.364431 0.291472 0.223758 0.190398 0.165911 0.120061 0.095512 0.069566","title":"Fit model and predict survival curves"},{"location":"examples/extrapolation_example.html#survival-curves-visualization","text":"import matplotlib.pyplot as plt plt.figure(figsize=(12,4), dpi=120) plt.plot( survival.columns, survival.iloc[42], 'k--', label='Survival' ) plt.title('Sample of predicted survival curves - $P(T>t)$') plt.legend() <matplotlib.legend.Legend at 0x7fc38026cef0> Notice that this predicted survival curve does not end at zero (cure fraction due to censored data). In some cases it might be useful to extrapolate our survival curves using specific strategies. xgbse.extrapolation implements a constant risk extrapolation strategy.","title":"Survival curves visualization"},{"location":"examples/extrapolation_example.html#extrapolation_1","text":"from xgbse.extrapolation import extrapolate_constant_risk # extrapolating predicted survival survival_ext = extrapolate_constant_risk(survival, 450, 11) survival_ext.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 15.0 30.0 45.0 60.0 75.0 90.0 105.0 120.0 135.0 150.0 ... 315.0 330.0 345.0 360.0 375.0 390.0 405.0 420.0 435.0 450.0 0 0.983502 0.951852 0.923277 0.900028 0.862270 0.799324 0.715860 0.687257 0.651314 0.610916 ... 0.112299 0.068213 0.032292 0.011915 0.003426 0.000768 0.000134 1.825794e-05 1.937124e-06 1.601799e-07 1 0.973506 0.917739 0.839154 0.710431 0.663119 0.558886 0.495204 0.364995 0.311628 0.299939 ... 0.026665 0.014641 0.005957 0.001796 0.000401 0.000066 0.000008 7.404100e-07 4.986652e-08 2.488634e-09 2 0.986894 0.959209 0.919768 0.889910 0.853239 0.777208 0.725381 0.649177 0.582569 0.531787 ... 0.088900 0.049982 0.021071 0.006660 0.001579 0.000281 0.000037 3.735612e-06 2.798762e-07 1.572266e-08 3 0.986753 0.955210 0.910354 0.857684 0.824301 0.769262 0.665805 0.624934 0.583592 0.537261 ... 0.067080 0.038160 0.016373 0.005299 0.001293 0.000238 0.000033 3.462388e-06 2.734946e-07 1.629408e-08 4 0.977348 0.940368 0.873695 0.804796 0.742655 0.632426 0.556008 0.521490 0.493577 0.458477 ... 0.050668 0.026879 0.010385 0.002923 0.000599 0.000089 0.000010 7.701555e-07 4.442463e-08 1.866412e-09 5 rows \u00d7 31 columns # plotting extrapolation # plt.figure(figsize=(12,4), dpi=120) plt.plot( survival.columns, survival.iloc[42], 'k--', label='Survival' ) plt.plot( survival_ext.columns, survival_ext.iloc[42], 'tomato', alpha=0.5, label='Extrapolated Survival' ) plt.title('Extrapolation of survival curves') plt.legend() <matplotlib.legend.Legend at 0x7fc3801842b0>","title":"Extrapolation"},{"location":"modules/base.html","text":"xgbse._base.XGBSEBaseEstimator \u00b6 \u00b6 Base class for all estimators in xgbse. Implements explainability through prototypes. get_neighbors ( self , query_data , index_data = None , query_id = None , index_id = None , n_neighbors = 30 ) \u00b6 Search for portotypes (size: n_neighbors) for each unit in a dataframe X. If units array is specified, comparables will be returned using its identifiers. If not, a dataframe of comparables indexes for each sample in X is returned. Parameters: Name Type Description Default query_data pd.DataFrame Dataframe of features to be used as input required query_id [pd.Series, np.array] Series or array of identification for each sample of query_data. Will be used in set_index if specified. None index_id [pd.Series, np.array] Series or array of identification for each sample of index_id. If specified, comparables will be returned using this identifier. None n_neighbors int Number of neighbors/comparables to be considered. 30 Returns: Type Description comps_df (pd.DataFrame) A dataframe of comparables/neighbors for each evaluated sample. If units identifier is specified, the output dataframe is converted to use units the proper identifier for each sample. The reference sample is considered to be the index of the dataframe and its comparables are its specific row values.","title":"XGBSEBaseEstimator"},{"location":"modules/base.html#xgbse_basexgbsebaseestimator","text":"","title":"xgbse._base.XGBSEBaseEstimator"},{"location":"modules/base.html#xgbse._base.XGBSEBaseEstimator","text":"Base class for all estimators in xgbse. Implements explainability through prototypes.","title":"xgbse._base.XGBSEBaseEstimator"},{"location":"modules/base.html#xgbse._base.XGBSEBaseEstimator.get_neighbors","text":"Search for portotypes (size: n_neighbors) for each unit in a dataframe X. If units array is specified, comparables will be returned using its identifiers. If not, a dataframe of comparables indexes for each sample in X is returned. Parameters: Name Type Description Default query_data pd.DataFrame Dataframe of features to be used as input required query_id [pd.Series, np.array] Series or array of identification for each sample of query_data. Will be used in set_index if specified. None index_id [pd.Series, np.array] Series or array of identification for each sample of index_id. If specified, comparables will be returned using this identifier. None n_neighbors int Number of neighbors/comparables to be considered. 30 Returns: Type Description comps_df (pd.DataFrame) A dataframe of comparables/neighbors for each evaluated sample. If units identifier is specified, the output dataframe is converted to use units the proper identifier for each sample. The reference sample is considered to be the index of the dataframe and its comparables are its specific row values.","title":"get_neighbors()"},{"location":"modules/bce.html","text":"xgbse._debiased_bce.XGBSEDebiasedBCE \u00b6 \u00b6 Train a set of logistic regressions on top of the leaf embedding produced by XGBoost, each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. Note Training and scoring of logistic regression models is efficient, being performed in parallel through joblib, so the model can scale to hundreds of thousands or millions of samples. However, if many windows are used and data is large, training of logistic regression models may become a bottleneck, taking more time than training of the underlying XGBoost model. Read more in How XGBSE works . __init__ ( self , xgb_params = None , lr_params = None , n_jobs =- 1 ) special \u00b6 Parameters: Name Type Description Default xgb_params Dict, None Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None lr_params Dict, None Parameters for Logistic Regression models. If not passed, the following default parameters will be used: DEFAULT_PARAMS_LR = {\"C\": 1e-3, \"max_iter\": 500} Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for more options. None n_jobs Int Number of CPU cores used to fit logistic regressions via joblib. -1 fit ( self , X , y , num_boost_round = 1000 , validation_data = None , early_stopping_rounds = None , verbose_eval = 0 , persist_train = False , index_id = None , time_bins = None ) \u00b6 Transform feature space by fitting a XGBoost model and returning its leaf indices. Leaves are transformed and considered as dummy variables to fit multiple logistic regression models to each evaluated time bin. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes False index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEDebiasedBCE Trained XGBSEDebiasedBCE instance predict ( self , X , return_interval_probs = False ) \u00b6 Predicts survival probabilities using the XGBoost + Logistic Regression pipeline. Parameters: Name Type Description Default X pd.DataFrame Dataframe of features to be used as input for the XGBoost model. required return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. False Returns: Type Description pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"XGBSEDebiasedBCE"},{"location":"modules/bce.html#xgbse_debiased_bcexgbsedebiasedbce","text":"","title":"xgbse._debiased_bce.XGBSEDebiasedBCE"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE","text":"Train a set of logistic regressions on top of the leaf embedding produced by XGBoost, each predicting survival at different user-defined discrete time windows. The classifiers remove individuals as they are censored, with targets that are indicators of surviving at each window. Note Training and scoring of logistic regression models is efficient, being performed in parallel through joblib, so the model can scale to hundreds of thousands or millions of samples. However, if many windows are used and data is large, training of logistic regression models may become a bottleneck, taking more time than training of the underlying XGBoost model. Read more in How XGBSE works .","title":"xgbse._debiased_bce.XGBSEDebiasedBCE"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.__init__","text":"Parameters: Name Type Description Default xgb_params Dict, None Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None lr_params Dict, None Parameters for Logistic Regression models. If not passed, the following default parameters will be used: DEFAULT_PARAMS_LR = {\"C\": 1e-3, \"max_iter\": 500} Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for more options. None n_jobs Int Number of CPU cores used to fit logistic regressions via joblib. -1","title":"__init__()"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.fit","text":"Transform feature space by fitting a XGBoost model and returning its leaf indices. Leaves are transformed and considered as dummy variables to fit multiple logistic regression models to each evaluated time bin. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes False index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEDebiasedBCE Trained XGBSEDebiasedBCE instance","title":"fit()"},{"location":"modules/bce.html#xgbse._debiased_bce.XGBSEDebiasedBCE.predict","text":"Predicts survival probabilities using the XGBoost + Logistic Regression pipeline. Parameters: Name Type Description Default X pd.DataFrame Dataframe of features to be used as input for the XGBoost model. required return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. False Returns: Type Description pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"predict()"},{"location":"modules/converters.html","text":"xgbse.converters.convert_to_structured \u00b6 \u00b6 Converts data in time (T) and event (E) format to a structured numpy array. Provides common interface to other libraries such as sksurv and sklearn. Parameters: Name Type Description Default T np.array Array of times required E np.array Array of events required Returns: Type Description np.array Structured array containing the boolean event indicator as first field, and time of event or time of censoring as second field xgbse.converters.convert_y \u00b6 \u00b6 Convert structured array y into an array of event indicators (E) and time of events (T). Parameters: Name Type Description Default y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required Returns: Type Description T ([np.array, pd.Series]) Time of events E ([np.array, pd.Series]): Binary event indicator","title":"converters"},{"location":"modules/converters.html#xgbseconvertersconvert_to_structured","text":"","title":"xgbse.converters.convert_to_structured"},{"location":"modules/converters.html#xgbse.converters.convert_to_structured","text":"Converts data in time (T) and event (E) format to a structured numpy array. Provides common interface to other libraries such as sksurv and sklearn. Parameters: Name Type Description Default T np.array Array of times required E np.array Array of events required Returns: Type Description np.array Structured array containing the boolean event indicator as first field, and time of event or time of censoring as second field","title":"xgbse.converters.convert_to_structured"},{"location":"modules/converters.html#xgbseconvertersconvert_y","text":"","title":"xgbse.converters.convert_y"},{"location":"modules/converters.html#xgbse.converters.convert_y","text":"Convert structured array y into an array of event indicators (E) and time of events (T). Parameters: Name Type Description Default y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required Returns: Type Description T ([np.array, pd.Series]) Time of events E ([np.array, pd.Series]): Binary event indicator","title":"xgbse.converters.convert_y"},{"location":"modules/extrapolation.html","text":"xgbse.extrapolation \u00b6 \u00b6 extrapolate_constant_risk ( survival , final_time , n_windows , lags =- 1 ) \u00b6 Extrapolate a survival curve assuming constant risk. Parameters: Name Type Description Default survival pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). required final_time Float Final time for extrapolation required n_windows Int Number of time windows to compute from last time window in survival to final_time required lags Int Lags to compute constant risk. if negative, will use the last \"lags\" values if positive, will remove the first \"lags\" values if 0, will use all values -1 Returns: Type Description pd.DataFrame Survival dataset with appended extrapolated windows","title":"extrapolation"},{"location":"modules/extrapolation.html#xgbseextrapolation","text":"","title":"xgbse.extrapolation"},{"location":"modules/extrapolation.html#xgbse.extrapolation","text":"","title":"xgbse.extrapolation"},{"location":"modules/extrapolation.html#xgbse.extrapolation.extrapolate_constant_risk","text":"Extrapolate a survival curve assuming constant risk. Parameters: Name Type Description Default survival pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). required final_time Float Final time for extrapolation required n_windows Int Number of time windows to compute from last time window in survival to final_time required lags Int Lags to compute constant risk. if negative, will use the last \"lags\" values if positive, will remove the first \"lags\" values if 0, will use all values -1 Returns: Type Description pd.DataFrame Survival dataset with appended extrapolated windows","title":"extrapolate_constant_risk()"},{"location":"modules/kaplan_neighs.html","text":"xgbse._kaplan_neighbors.XGBSEKaplanNeighbors \u00b6 \u00b6 Convert xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-ocurred the most at the ensemble terminal nodes. Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator. Note We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. This method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search). Read more in How XGBSE works . __init__ ( self , xgb_params = None , n_neighbors = 30 , radius = None ) special \u00b6 Parameters: Name Type Description Default xgb_params Dict Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None n_neighbors Int Number of neighbors for computing KM estimates 30 radius Float If set, uses a radius around the point for neighbors search None fit ( self , X , y , num_boost_round = 1000 , validation_data = None , early_stopping_rounds = None , verbose_eval = 0 , persist_train = True , index_id = None , time_bins = None ) \u00b6 Transform feature space by fitting a XGBoost model and outputting its leaf indices. Build search index in the new space to allow nearest neighbor queries at scoring time. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Design matrix to fit XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes True index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEKaplanNeighbors Fitted instance of XGBSEKaplanNeighbors predict ( self , X , time_bins = None , return_ci = False , ci_width = 0.683 , return_interval_probs = False ) \u00b6 Make queries to nearest neighbor search index build on the transformed XGBoost space. Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators. Parameters: Name Type Description Default X pd.DataFrame Dataframe with samples to generate predictions required time_bins np.array Specified time windows to use when making survival predictions None return_ci Bool Whether to return confidence intervals via the Exponential Greenwood formula False ci_width Float Width of confidence interval 0.683 return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. False Returns: Type Description (pd.DataFrame) A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): Upper confidence interval for the survival probability values lower_ci (np.array): Lower confidence interval for the survival probability values","title":"XGBSEKaplanNeighbors"},{"location":"modules/kaplan_neighs.html#xgbse_kaplan_neighborsxgbsekaplanneighbors","text":"","title":"xgbse._kaplan_neighbors.XGBSEKaplanNeighbors"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors","text":"Convert xgboost into a nearest neighbor model, where we use hamming distance to define similar elements as the ones that co-ocurred the most at the ensemble terminal nodes. Then, at each neighbor-set compute survival estimates with the Kaplan-Meier estimator. Note We recommend using dart as the booster to prevent any tree to dominate variance in the ensemble and break the leaf co-ocurrence similarity logic. This method can be very expensive at scales of hundreds of thousands of samples, due to the nearest neighbor search, both on training (construction of search index) and scoring (actual search). Read more in How XGBSE works .","title":"xgbse._kaplan_neighbors.XGBSEKaplanNeighbors"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.__init__","text":"Parameters: Name Type Description Default xgb_params Dict Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None n_neighbors Int Number of neighbors for computing KM estimates 30 radius Float If set, uses a radius around the point for neighbors search None","title":"__init__()"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.fit","text":"Transform feature space by fitting a XGBoost model and outputting its leaf indices. Build search index in the new space to allow nearest neighbor queries at scoring time. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Design matrix to fit XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes True index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEKaplanNeighbors Fitted instance of XGBSEKaplanNeighbors","title":"fit()"},{"location":"modules/kaplan_neighs.html#xgbse._kaplan_neighbors.XGBSEKaplanNeighbors.predict","text":"Make queries to nearest neighbor search index build on the transformed XGBoost space. Compute a Kaplan-Meier estimator for each neighbor-set. Predict the KM estimators. Parameters: Name Type Description Default X pd.DataFrame Dataframe with samples to generate predictions required time_bins np.array Specified time windows to use when making survival predictions None return_ci Bool Whether to return confidence intervals via the Exponential Greenwood formula False ci_width Float Width of confidence interval 0.683 return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. False Returns: Type Description (pd.DataFrame) A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): Upper confidence interval for the survival probability values lower_ci (np.array): Lower confidence interval for the survival probability values","title":"predict()"},{"location":"modules/kaplan_tree.html","text":"xgbse._kaplan_neighbors.XGBSEKaplanTree \u00b6 \u00b6 Single tree implementation as a simplification to XGBSEKaplanNeighbors . Instead of doing nearest neighbor searches, fits a single tree via xgboost and calculates KM curves at each of its leaves. Note It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. Read more in How XGBSE works . __init__ ( self , xgb_params = None ) special \u00b6 Parameters: Name Type Description Default xgb_params Dict Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS_TREE = { \"objective\": \"survival:cox\", \"eval_metric\": \"cox-nloglik\", \"tree_method\": \"exact\", \"max_depth\": 100, \"booster\": \"dart\", \"subsample\": 1.0, \"min_child_weight\": 30, \"colsample_bynode\": 1.0, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None fit ( self , X , y , persist_train = True , index_id = None , time_bins = None , ci_width = 0.683 , ** xgb_kwargs ) \u00b6 Fit a single decision tree using xgboost. For each leaf in the tree, build a Kaplan-Meier estimator. Note Differently from XGBSEKaplanNeighbors , in XGBSEKaplanTree , the width of the confidence interval ( ci_width ) must be specified at fit time. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Design matrix to fit XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required persist_train Bool Whether or not to persist training data to use explainability through prototypes True index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None ci_width Float Width of confidence interval 0.683 Returns: Type Description XGBSEKaplanTree Trained instance of XGBSEKaplanTree predict ( self , X , return_ci = False , return_interval_probs = False ) \u00b6 Run samples through tree until terminal nodes. Predict the Kaplan-Meier estimator associated to the leaf node each sample ended into. Parameters: Name Type Description Default X pd.DataFrame Data frame with samples to generate predictions required return_ci Bool Whether to return confidence intervals via the Exponential Greenwood formula False return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. False Returns: Type Description preds_df (pd.DataFrame) A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): Upper confidence interval for the survival probability values lower_ci (np.array): Lower confidence interval for the survival probability values","title":"XGBSEKaplanTree"},{"location":"modules/kaplan_tree.html#xgbse_kaplan_neighborsxgbsekaplantree","text":"","title":"xgbse._kaplan_neighbors.XGBSEKaplanTree"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree","text":"Single tree implementation as a simplification to XGBSEKaplanNeighbors . Instead of doing nearest neighbor searches, fits a single tree via xgboost and calculates KM curves at each of its leaves. Note It is by far the most efficient implementation, able to scale to millions of examples easily. At fit time, the tree is built and all KM curves are pre-calculated, so that at scoring time a simple query will suffice to get the model's estimates. Read more in How XGBSE works .","title":"xgbse._kaplan_neighbors.XGBSEKaplanTree"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.__init__","text":"Parameters: Name Type Description Default xgb_params Dict Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS_TREE = { \"objective\": \"survival:cox\", \"eval_metric\": \"cox-nloglik\", \"tree_method\": \"exact\", \"max_depth\": 100, \"booster\": \"dart\", \"subsample\": 1.0, \"min_child_weight\": 30, \"colsample_bynode\": 1.0, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None","title":"__init__()"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.fit","text":"Fit a single decision tree using xgboost. For each leaf in the tree, build a Kaplan-Meier estimator. Note Differently from XGBSEKaplanNeighbors , in XGBSEKaplanTree , the width of the confidence interval ( ci_width ) must be specified at fit time. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Design matrix to fit XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required persist_train Bool Whether or not to persist training data to use explainability through prototypes True index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None ci_width Float Width of confidence interval 0.683 Returns: Type Description XGBSEKaplanTree Trained instance of XGBSEKaplanTree","title":"fit()"},{"location":"modules/kaplan_tree.html#xgbse._kaplan_neighbors.XGBSEKaplanTree.predict","text":"Run samples through tree until terminal nodes. Predict the Kaplan-Meier estimator associated to the leaf node each sample ended into. Parameters: Name Type Description Default X pd.DataFrame Data frame with samples to generate predictions required return_ci Bool Whether to return confidence intervals via the Exponential Greenwood formula False return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. False Returns: Type Description preds_df (pd.DataFrame) A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci (np.array): Upper confidence interval for the survival probability values lower_ci (np.array): Lower confidence interval for the survival probability values","title":"predict()"},{"location":"modules/meta.html","text":"xgbse._meta.XGBSEBootstrapEstimator \u00b6 \u00b6 Bootstrap meta-estimator for XGBSE models: allows for confidence interval estimation for XGBSEDebiasedBCE and XGBSEStackedWeibull provides variance stabilization for all models, specially for XGBSEKaplanTree Performs simple bootstrap with sample size equal to training set size. __init__ ( self , base_estimator , n_estimators = 10 , random_state = 42 ) special \u00b6 Parameters: Name Type Description Default base_estimator XGBSEBaseEstimator Base estimator for bootstrap procedure required n_estimators int Number of estimators to fit in bootstrap procedure 10 random_state int Random state for resampling function 42 fit ( self , X , y , ** kwargs ) \u00b6 Fit several (base) estimators and store them. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required **kwargs Keyword arguments to be passed to .fit() method of base_estimator {} Returns: Type Description XGBSEBootstrapEstimator Trained instance of XGBSEBootstrapEstimator predict ( self , X , return_ci = False , ci_width = 0.683 , return_interval_probs = False ) \u00b6 Predicts survival as given by the base estimator. A survival function, its upper and lower confidence intervals can be returned for each sample of the dataframe X. Parameters: Name Type Description Default X pd.DataFrame data frame with samples to generate predictions required return_ci Bool whether to include confidence intervals False ci_width Float width of confidence interval 0.683 Returns: Type Description ([(pd.DataFrame, np.array, np.array), pd.DataFrame]) preds_df: A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci: Upper confidence interval for the survival probability values lower_ci: Lower confidence interval for the survival probability values","title":"XGBSEBootstrapEstimator"},{"location":"modules/meta.html#xgbse_metaxgbsebootstrapestimator","text":"","title":"xgbse._meta.XGBSEBootstrapEstimator"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator","text":"Bootstrap meta-estimator for XGBSE models: allows for confidence interval estimation for XGBSEDebiasedBCE and XGBSEStackedWeibull provides variance stabilization for all models, specially for XGBSEKaplanTree Performs simple bootstrap with sample size equal to training set size.","title":"xgbse._meta.XGBSEBootstrapEstimator"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.__init__","text":"Parameters: Name Type Description Default base_estimator XGBSEBaseEstimator Base estimator for bootstrap procedure required n_estimators int Number of estimators to fit in bootstrap procedure 10 random_state int Random state for resampling function 42","title":"__init__()"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.fit","text":"Fit several (base) estimators and store them. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required **kwargs Keyword arguments to be passed to .fit() method of base_estimator {} Returns: Type Description XGBSEBootstrapEstimator Trained instance of XGBSEBootstrapEstimator","title":"fit()"},{"location":"modules/meta.html#xgbse._meta.XGBSEBootstrapEstimator.predict","text":"Predicts survival as given by the base estimator. A survival function, its upper and lower confidence intervals can be returned for each sample of the dataframe X. Parameters: Name Type Description Default X pd.DataFrame data frame with samples to generate predictions required return_ci Bool whether to include confidence intervals False ci_width Float width of confidence interval 0.683 Returns: Type Description ([(pd.DataFrame, np.array, np.array), pd.DataFrame]) preds_df: A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities. upper_ci: Upper confidence interval for the survival probability values lower_ci: Lower confidence interval for the survival probability values","title":"predict()"},{"location":"modules/metrics.html","text":"xgbse.metrics \u00b6 \u00b6 approx_brier_score ( y_true , survival , aggregate = 'mean' ) \u00b6 Estimate brier score for all survival time windows. Aggregate scores for an approximate integrated brier score estimate. Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number B inary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required aggregate [string, None] How to aggregate brier scores from different time windows: mean takes simple average None returns full list of brier scores for each time window 'mean' Returns: Type Description [Float, np.array] single value if aggregate is 'mean' np.array if aggregate is None concordance_index ( y_true , survival , risk_strategy = 'mean' , which_window = None ) \u00b6 Compute the C-index for a structured array of ground truth times and events and a predicted survival curve using different strategies for estimating risk from it. Note Computation of the C-index is \\(\\mathcal{O}(n^2)\\) . Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required risk_strategy string Strategy to compute risks from the survival curve. For a given sample: mean averages probabilities across all times window : lets user choose on of the time windows available (by which_window argument) and uses probabilities of this specific window midpoint : selects the most central window of index int(survival.columns.shape[0]/2) and uses probabilities of this specific window precomputed : assumes user has already calculated risk. The survival argument is assumed to contain an array of risks instead 'mean' which_window object Which window to use when risk_strategy is 'window'. Should be one of the columns of the dataframe. Will raise ValueError if column is not present None Returns: Type Description Float Concordance index for y_true and survival dist_calibration_score ( y_true , survival , n_bins = 10 , returns = 'pval' ) \u00b6 Estimate D-Calibration for the survival predictions. Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required n_bins Int Number of bins to equally divide the [0, 1] interval 10 returns string What information to return from the function: statistic returns the chi squared test statistic pval returns the chi squared test p value max_deviation returns the maximum percentage deviation from the expected value, calculated as abs(expected_percentage - real_percentage) , where expected_percentage = 1.0/n_bins histogram returns the full calibration histogram per bin all returns all of the above in a dictionary 'pval' Returns: Type Description [Float, np.array, Dict] Single value if returns is in `['statistic','pval','max_deviation']`` np.array if returns is 'histogram' dict if returns is 'all'","title":"metrics"},{"location":"modules/metrics.html#xgbsemetrics","text":"","title":"xgbse.metrics"},{"location":"modules/metrics.html#xgbse.metrics","text":"","title":"xgbse.metrics"},{"location":"modules/metrics.html#xgbse.metrics.approx_brier_score","text":"Estimate brier score for all survival time windows. Aggregate scores for an approximate integrated brier score estimate. Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number B inary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required aggregate [string, None] How to aggregate brier scores from different time windows: mean takes simple average None returns full list of brier scores for each time window 'mean' Returns: Type Description [Float, np.array] single value if aggregate is 'mean' np.array if aggregate is None","title":"approx_brier_score()"},{"location":"modules/metrics.html#xgbse.metrics.concordance_index","text":"Compute the C-index for a structured array of ground truth times and events and a predicted survival curve using different strategies for estimating risk from it. Note Computation of the C-index is \\(\\mathcal{O}(n^2)\\) . Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required risk_strategy string Strategy to compute risks from the survival curve. For a given sample: mean averages probabilities across all times window : lets user choose on of the time windows available (by which_window argument) and uses probabilities of this specific window midpoint : selects the most central window of index int(survival.columns.shape[0]/2) and uses probabilities of this specific window precomputed : assumes user has already calculated risk. The survival argument is assumed to contain an array of risks instead 'mean' which_window object Which window to use when risk_strategy is 'window'. Should be one of the columns of the dataframe. Will raise ValueError if column is not present None Returns: Type Description Float Concordance index for y_true and survival","title":"concordance_index()"},{"location":"modules/metrics.html#xgbse.metrics.dist_calibration_score","text":"Estimate D-Calibration for the survival predictions. Parameters: Name Type Description Default y_true structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required survival [pd.DataFrame, np.array] A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If risk_strategy is 'precomputed', is an array with representing risks for each sample. required n_bins Int Number of bins to equally divide the [0, 1] interval 10 returns string What information to return from the function: statistic returns the chi squared test statistic pval returns the chi squared test p value max_deviation returns the maximum percentage deviation from the expected value, calculated as abs(expected_percentage - real_percentage) , where expected_percentage = 1.0/n_bins histogram returns the full calibration histogram per bin all returns all of the above in a dictionary 'pval' Returns: Type Description [Float, np.array, Dict] Single value if returns is in `['statistic','pval','max_deviation']`` np.array if returns is 'histogram' dict if returns is 'all'","title":"dist_calibration_score()"},{"location":"modules/stacked_weibull.html","text":"xgbse._stacked_weibull.XGBSEStackedWeibull \u00b6 \u00b6 Perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable. Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves). Note As we're stacking XGBoost with a single, one-variable parametric model (as opposed to XGBSEDebiasedBCE ), the model can be much faster (especially in training). We also have better extrapolation capabilities, as opposed to the cure fraction problem in XGBSEKaplanNeighbors and XGBSEKaplanTree . However, we also have stronger assumptions about the shape of the survival curve. Read more in How XGBSE works . __init__ ( self , xgb_params = None , weibull_params = None ) special \u00b6 Parameters: Name Type Description Default xgb_params Dict, None Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None weibull_params Dict Parameters for Weibull Regerssion model. If not passed, will use the default parameters as shown in the Lifelines documentation. Check https://lifelines.readthedocs.io/en/latest/fitters/regression/WeibullAFTFitter.html for more options. None fit ( self , X , y , num_boost_round = 1000 , validation_data = None , early_stopping_rounds = None , verbose_eval = 0 , persist_train = False , index_id = None , time_bins = None ) \u00b6 Fit XGBoost model to predict a value that is interpreted as a risk metric. Fit Weibull Regression model using risk metric as only independent variable. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes False index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEStackedWeibull Trained XGBSEStackedWeibull instance predict ( self , X , return_interval_probs = False ) \u00b6 Predicts survival probabilities using the XGBoost + Weibull AFT stacking pipeline. Parameters: Name Type Description Default X pd.DataFrame Dataframe of features to be used as input for the XGBoost model. required return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. False Returns: Type Description pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"XGBSEStackedWeibull"},{"location":"modules/stacked_weibull.html#xgbse_stacked_weibullxgbsestackedweibull","text":"","title":"xgbse._stacked_weibull.XGBSEStackedWeibull"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull","text":"Perform stacking of a XGBoost survival model with a Weibull AFT parametric model. The XGBoost fits the data and then predicts a value that is interpreted as a risk metric. This risk metric is fed to the Weibull regression which uses it as its only independent variable. Thus, we can get the benefit of XGBoost discrimination power alongside the Weibull AFT statistical rigor (e.g. calibrated survival curves). Note As we're stacking XGBoost with a single, one-variable parametric model (as opposed to XGBSEDebiasedBCE ), the model can be much faster (especially in training). We also have better extrapolation capabilities, as opposed to the cure fraction problem in XGBSEKaplanNeighbors and XGBSEKaplanTree . However, we also have stronger assumptions about the shape of the survival curve. Read more in How XGBSE works .","title":"xgbse._stacked_weibull.XGBSEStackedWeibull"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.__init__","text":"Parameters: Name Type Description Default xgb_params Dict, None Parameters for XGBoost model. If not passed, the following default parameters will be used: DEFAULT_PARAMS = { \"objective\": \"survival:aft\", \"eval_metric\": \"aft-nloglik\", \"aft_loss_distribution\": \"normal\", \"aft_loss_distribution_scale\": 1, \"tree_method\": \"hist\", \"learning_rate\": 5e-2, \"max_depth\": 8, \"booster\": \"dart\", \"subsample\": 0.5, \"min_child_weight\": 50, \"colsample_bynode\": 0.5, } Check https://xgboost.readthedocs.io/en/latest/parameter.html for more options. None weibull_params Dict Parameters for Weibull Regerssion model. If not passed, will use the default parameters as shown in the Lifelines documentation. Check https://lifelines.readthedocs.io/en/latest/fitters/regression/WeibullAFTFitter.html for more options. None","title":"__init__()"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.fit","text":"Fit XGBoost model to predict a value that is interpreted as a risk metric. Fit Weibull Regression model using risk metric as only independent variable. Parameters: Name Type Description Default X [pd.DataFrame, np.array] Features to be used while fitting XGBoost model required y structured array(numpy.bool_, numpy.number Binary event indicator as first field, and time of event or time of censoring as second field. required num_boost_round Int Number of boosting iterations. 1000 validation_data Tuple Validation data in the format of a list of tuples [(X, y)] if user desires to use early stopping None early_stopping_rounds Int Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. See xgboost.train documentation. None verbose_eval [Bool, Int] Level of verbosity. See xgboost.train documentation. 0 persist_train Bool Whether or not to persist training data to use explainability through prototypes False index_id pd.Index User defined index if intended to use explainability through prototypes None time_bins np.array Specified time windows to use when making survival predictions None Returns: Type Description XGBSEStackedWeibull Trained XGBSEStackedWeibull instance","title":"fit()"},{"location":"modules/stacked_weibull.html#xgbse._stacked_weibull.XGBSEStackedWeibull.predict","text":"Predicts survival probabilities using the XGBoost + Weibull AFT stacking pipeline. Parameters: Name Type Description Default X pd.DataFrame Dataframe of features to be used as input for the XGBoost model. required return_interval_probs Bool Boolean indicating if interval probabilities are supposed to be returned. If False the cumulative survival is returned. Default is False. False Returns: Type Description pd.DataFrame A dataframe of survival probabilities for all times (columns), from a time_bins array, for all samples of X (rows). If return_interval_probs is True, the interval probabilities are returned instead of the cumulative survival probabilities.","title":"predict()"}]}